<?xml version="1.0" encoding="UTF-8"?> <?xml-model href="../../../docbook-customization/src/docbkx-stylesheet/common/komet.rnc" type="application/relax-ng-compact-syntax"?>
<chapter xmlns="http://docbook.org/ns/docbook" xmlns:xi="http://www.w3.org/2001/XInclude"
  xmlns:xlink="http://www.w3.org/1999/xlink" version="5.1">
  <title>Conclusion</title>
  <para>ANF has implications on clinical data quality, clinical decision support, patient safety and
    population health. </para>
  <section>
    <title> Implications for Data Quality </title>
    <para>Information systems record and manage clinical statements using a variety of standard or
      ad-hoc models. However, both treatment and analysis of clinical statements require consistency
      not only at the format level (e.g. CDA, FHIR, V2) but also the content model (i.e. an instance
      of a DCM, CIMI model, etc.). In most cases the data quality is the greatest obstacle to
      analysis. Analysis Normal aims to minimize data quality challenges and provide a common format
      with semantic clarity to allow for a meaningful secondary use of clinical data. </para>
    <para>ANF has beneficial implications on the following data quality categories based on a
      harmonized data quality framework by Khan et al: (Khan)</para>
    <para>
      <itemizedlist>
        <listitem>
          <para><emphasis role="bold">Conformance</emphasis>: Conformance focuses on data quality features that
            describe the compliance of the representation of data against internal or external
            formatting, relational, or computational definitions. ANF has explicit design structures
            for syntactic and structural constraints of clinical data, with specific formats and
            allowed values for particular data elements. As described in this document, ANF focuses
            on subcategories of Conformance data quality: value conformance, relational conformance,
            and computational conformance. </para>
          <itemizedlist>
            <listitem>
              <para>Value Conformance: Value conformance seeks to determine if recorded data
                elements are in agreement with a prespecified, constraint-driven data architecture.
                Internal data constraints are typically imposed by a formal data model, which
                specifies expectations for data types, data domains and allowed values, and data
                formats.</para>
            </listitem>
            <listitem>
              <para>Relational Conformance: Relational conformance seeks to determine if the
                recorded data elements are in agreement with additional structural constraints
                imposed by the physical database structures that store data values. </para>
            </listitem>

            <?oxy_comment_start author="jcoyle" timestamp="20190622T100303-0400" comment="ANF is abstracted above a relational database, in fact no relational database is required.  Any kind of datastore will work."?><?oxy_comment_end?>

            <listitem>
              <para>Computational Conformance: Computational conformance seeks to determine if
                computations used to create derived values from existing variables yield the
                intended results either within a data set (Verification) or between data sets
                (Validation), when programs are based on identical specifications. Computational
                conformance focuses on the correctness of the output value of calculations against
                technical functional specifications.</para>
            </listitem>
          </itemizedlist>
        </listitem>
        <listitem>
          <para><emphasis role="bold">Completeness</emphasis>: Completeness focuses on features that
            describe the frequencies of data attributes present in a data set without reference to
            data values. Completeness measures assess the absence of data at a single moment over
            time or when measured at multiple moments over time.</para>
        </listitem>
        <listitem>
          <para><emphasis role="bold">Plausbility:</emphasis> Plausibility focuses on features that
            describe the believability or truthfulness of data values. For this category,
            plausibility is determined by a variable’s value, when a value is placed within the
            context of another variable (i.e., two independent variables assessing the same
            construct), or a temporal sequence or state transition (i.e., patient follow-up
            treatment for a disease must be preceded by a corresponding diagnosis).</para>
          <itemizedlist>
            <listitem>
              <para>Uniqueness Plausibility<emphasis role="bold">:</emphasis> The Uniqueness
                subcategory seeks to determine if objects (entities, observations, facts) appear
                multiple times in settings where they should not be duplicated or cannot be
                distinguished within a database (Verification) or when compared with an external
                reference (Validation). Duplication frequently occurs when disparate data streams
                that contain overlapping objects are combined. Data extraction errors, such as
                incomplete relational join conditions, can also generate duplicate records.</para>
            </listitem>

            <?oxy_comment_start author="jcoyle" timestamp="20190622T100303-0400" comment="Can we change database to data repository or something more generic"?><?oxy_comment_end?>

            <listitem>
              <para>Atemporal Plausibility: Atemporal Plausibility seeks to determine if observed
                data values, distributions, or densities agree with local or “common” knowledge
                (Verification) or from comparisons with external sources that are deemed to be
                trusted or relative gold standards (Validation).</para>
            </listitem>
            <listitem>
              <para>Temporal Plausibility: Temporal plausibility seeks to determine if time-varying
                variables change values as expected based on known temporal properties or across one
                or more external comparators or gold standards. Temporal properties that establish
                expectations in this subcategory include temporal stability (do values vary over
                time as expected), temporal continuity (do values persist over time as expected),
                state transitions (do sequences of events occur as expected), and temporal
                dependencies between time-varying variables.</para>
            </listitem>
          </itemizedlist>
        </listitem>
      </itemizedlist>
    </para>
  </section>
  <section>
    <title>Implications on Clinical Decision Support</title>
    <para>A 2012 Literature Review commissioned by The Agency for Healthcare Research and Quality
      (AHRQ) found evidence showing that CDS had positive impact on process measures and increasing
      user knowledge relevant to a medical condition. (Lobach et al., 2012) </para>
    <para>Additional studies show that well-executed CDS can:</para>
    <itemizedlist>
      <listitem>
        <para>reduce adverse drug-drug interaction events and medication errors (Smithburger et al.,
          2011; Sonnichsen et al., 2016) (Fritz et al., 2012); </para>
      </listitem>
      <listitem>
        <para>decrease unnecessary lab testing (Felcher et al., 2017);</para>
      </listitem>
      <listitem>
        <para>reduce cardiovascular risk in patients with type 2 diabetes (Cleveringa et al., 2008);
        </para>
      </listitem>
      <listitem>
        <para>improve practitioner performance (Garg et al., 2005); </para>
      </listitem>
      <listitem>
        <para>increase cardiovascular disease risk assessment in routine primary care practice
          (Wells et al., 2008); </para>
      </listitem>
      <listitem>
        <para>improve public health outcomes associated with outbreaks of foodborne illness (Wu et
          al., 2012); </para>
      </listitem>
      <listitem>
        <para>and, produce cost savings associated with hospital-based pharmacy interventions
          (Calloway et al., 2013).</para>
      </listitem>
    </itemizedlist>
    <para>Taken together, the available evidence shows that CDS —when implemented in the right
      context, and when governed with formal management—can reduce errors, improve the quality of
      care, reduce cost, and ease the cognitive burden on health care providers. (ONC, NAM Report)
      As a result, the impetus for achieving standardized, widespread adoption of CDS across health
      systems is clear.</para>
    <para>A report entitled “<emphasis role="italic">Optimizing Strategies for Clinical Decision
        Support: Summary of a Meeting Series</emphasis>” was produced out of the collaboration
      between the ONC and the National Academy of Medicine (NAM). The report states that there are
      at least four important technical challenges to sharing and therefore standardizing
      implementations of CDS content: (ONC, NAM Report)</para>
    <para><emphasis role="bold">(1) insufficient standardization of patient data representation;
      </emphasis></para>
    <para>(2) insufficient standardization of CDS knowledge representation; </para>
    <para>(3) insufficient standardization of CDS integration mechanisms; </para>
    <para>(4) a need to align with broader standardization initiatives. </para>
    <para>One of the reasons that CDS interventions are difficult to implement between health care
      systems is because disparate EHR systems and health care systems utilize different underlying
      patient data models and clinical statement representation mechanisms. Even distinct
      instantiations of use of the same EHR systems differ in how they encode patient data and in
      how they represent clinical statements. The ONC and NAM report states that "[b]ecause CDS
      relies on inferencing using patient data, this heterogeneity in patient data representation
      poses an immense obstacle to sharing CDS." (ONC, NAM Report)</para>
    <para>ANF aims to reduce the variability of how clinical data within the value sets and CDS
      rules are inputted into EHR systems and modeled/stored in data repositories. The
      standardization of clinical observations in a manner that supports automated processing
      requires a formal clinical statement model, such as ANF. The most important requirements of
      such a statement model are that (1) it can represent any clinician-specified observation
      accurately and precisely and (2) it can support automated query and retrieval operations
      correctly and efficiently.</para>

<?oxy_comment_start author="jcoyle" timestamp="20190622T100303-0400" comment="This could be worded more clearly -> ANF aims to reduce the variability of how clinical data within the value sets and CDS rules are inputted into EHR systems and modeled/stored in data repositories."?><?oxy_comment_end?>


    <para>Importantly, ANF supports the ability to enable clinicians who use SCT to express
      observations that do not appear as pre-defined concepts in the terminology, thereby vastly
      increasing the expressive power of clinical terminologies. For example, a clinician could
      document that a patient has “bacterial pneumonia caused by methicillin-resistant Staph.
      Aureus” by combining the pre-existing concept “bacterial pneumonia” with the pre-existing
      concept “Methicillin Resistant Staph. Aureus” and specifying that the latter is the “causative
      agent” of the former. The patient’s medical record would then contain an entry consisting of
      the following expression: </para>
    <para>Bacterial Pneumonia (ConceptID = 53084003) : Causative Agent (ConceptID=246075003) =
      Methicillin Resistant Staph. Aureus (ConceptID=115329001)</para>
    <para>If specified correctly, post-coordinated expressions also support subsumption testing.
      Hence, the patient whose record contains the expression above would also be identified by the
      query “find all patients with a diagnosis of any infectious disease (Infectious Disease :
      ConceptID = 40733004) in their record.”</para>
  </section>
  <section>
    <title>Implications on Population Health</title>
    <para>Electronic clinical quality measures (eCQMs) and CDS alerts are triggered by clinical data
      that is represented in data repositories by clinical statements represented by detailed
      clinical models with data elements encoded by standards-based clinical terminologies. Because
      these measures and alerts intend to promote evidence-based clinical processes, variations in
      clinical data caused by having inaccurate, incomplete, or antiquated implementations of
      underlying logical models may impact the ability of clinicians to assess care and improve
      quality. Jean-Jacques et al. showed that health information technology-supported quality
      improvement (QI) initiatives can decrease disparities for some chronic disease management and
      preventive measures QI. Data-driven QI efforts rely heavily on patient-level data generated by
      eCQM reports or CDS alerts, which are dependent upon standards-based encoded clinical data. If
      clinicians rely on inaccurate implementations of eCQMs and CDS, then they may have
      lists/alerts with patients intended to be excluded from a measure/alert, and may therefore,
      target inappropriate patients for therapies, such as recommending aspirin use for someone at
      high-risk for a fatal bleeding event.</para>
    <para>Cholan et al showed that variations in implementations of eCQM specifications for
      cardiovascular event prevention could result in potential lives saved or harms avoided in
      quality improvement activities. For aspirin use for secondary prevention of heart attacks,
      Number-Needed-to- Treat (NNT) statistics show that of patients with known cardiovascular risk
      who took aspirin, 1.3% were helped by preventing a non-fatal heart attack, and 0.25% were
      harmed by a major bleeding event. In the Cholan et al study, 121 (92%) of the patients were
      inappropriately included in a measure’s denominator. These patients were also taking an
      anticoagulant medication, so the Number-Needed-to-Harm (NNH) statistic for this subset of
      patients is likely much higher, and for this study, 1 to 2 people may have been harmed if the
      inaccurate implementation persisted, as Hansen et. al showed that patients with combinations
      of aspirin, warfarin, and clopidogrel are associated with up to a three-fold higher risk of
      bleeding for patients on dual therapy and triple therapy. With another measure for statin
      therapy, 1 in 21 people have a repeat heart attack, stroke or death avoided, so even 10 missed
      people have significant risk of events. Similarly, 10% are harmed by muscle damage or pain, or
      ~1 of the 14 inappropriately included in the Cholan et al study. Even in the small Cholan et
      al study with data from two primary care clinics, failure to include or exclude patients could
      have led to real harm. </para>
    <para>With eCQM implementation and QI infrastructure increasing, the problem of having, and
      using, inaccurate eCQM implementations or CDS implementations could have significant potential
      negative impact on population health by not avoiding events, and avoiding harms for patients.
      ANF reduces these erroneous implementations. Without a precise logical model for clinical data
      like ANF, comparability of eCQMs for payment programs and utility of CQM data for targeted
      quality improvement may be limited.</para>
  </section>
  <section>
    <title>Summary</title>
    <para>In conclusion, Analysis Normal Form (ANF) presents a simple reproducible approach
      to modeling clinical statements specifically for data analysis. It reduces clinical statements to two
      types, Performance of Action and Request for Action, both clinical statement types with
      topics. ANF is compatible with other work in statement representation models such as the CIMI
      Clinical Statement approach, with it's focus on more traditional complex structured trees, whereas ANF focuses on structuring that data in a way for CDS systems
      can extract that data in an unambiguous way.</para>
    <para><?oxy_comment_start author="bechang" timestamp="20190426T225012-0600" comment="Need Ioana to expand upon what would be next after successful balloting of this whitepaper in September 2019"?>We
      believe that ANF represents an approach to statement representation that needs to next be
      refined into a normative set of editorial guidelines that can eventually also be expanded upon
      into the latest standards, such as creating an HL7 FHIR Implementation
      Guide<?oxy_comment_end?>.</para>
  </section>
  
  
 
</chapter>
